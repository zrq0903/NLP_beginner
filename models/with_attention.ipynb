{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-23T17:36:34.288420Z"
    }
   },
   "source": [
    "# Importing the libraries\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import embedding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from models.negative_sampling import vocab"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:21.843964300Z",
     "start_time": "2024-11-23T11:08:20.877289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_input_len = 46\n",
    "max_output_len = 46\n",
    "sos_token = 0\n",
    "eos_token = 1\n",
    "pad_token = 2\n",
    "batch_size = 16"
   ],
   "id": "f15106d1f3a2dcd",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:21.924749Z",
     "start_time": "2024-11-23T11:08:20.916691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"..\\dataset\\ELRC-417-Swedish_Work_Environ.en-fr.en\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "src_sentences = []\n",
    "#print(len(lines))\n",
    "for line in lines[1:-1]:\n",
    "    src_sentences.append(line.lower().strip())\n",
    "\n",
    "with open(r\"..\\dataset\\ELRC-417-Swedish_Work_Environ.en-fr.fr\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "trg_sentences = []\n",
    "for line in lines[1:-1]:\n",
    "    trg_sentences.append(line.lower().strip())"
   ],
   "id": "b513fb6c0c5c622e",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:21.926742900Z",
     "start_time": "2024-11-23T11:08:20.963556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.word_tokenize(src_sentences[0])\n",
    "max(len(nltk.word_tokenize(sentence)) for sentence in trg_sentences)"
   ],
   "id": "274bd48f562452b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:22.017500400Z",
     "start_time": "2024-11-23T11:08:21.026043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "src_word_list = []\n",
    "for sentence in src_sentences:\n",
    "    src_word_list.extend([word.lower() for word in sentence[:-1].split()])\n",
    "#len(src_word_list)\n",
    "src_vocab = set(src_word_list)\n",
    "\n",
    "src_vocab = list(src_vocab)\n",
    "src_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\n",
    "trg_word_list = []\n",
    "for sentence in trg_sentences:\n",
    "    trg_word_list.extend([word.lower() for word in sentence[:-1].split()])\n",
    "trg_vocab = list(set(trg_word_list))\n",
    "trg_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\n",
    "trg_vocab\n",
    "'''"
   ],
   "id": "8181c86554511dfb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsrc_word_list = []\\nfor sentence in src_sentences:\\n    src_word_list.extend([word.lower() for word in sentence[:-1].split()])\\n#len(src_word_list)\\nsrc_vocab = set(src_word_list)\\n\\nsrc_vocab = list(src_vocab)\\nsrc_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\\ntrg_word_list = []\\nfor sentence in trg_sentences:\\n    trg_word_list.extend([word.lower() for word in sentence[:-1].split()])\\ntrg_vocab = list(set(trg_word_list))\\ntrg_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\\ntrg_vocab\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "maybe try nltk.tokenize tomorrow",
   "id": "e1e2433d30df8280"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:22.042433800Z",
     "start_time": "2024-11-23T11:08:21.072905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "src_word_to_index = {w:i for i,w in enumerate(src_vocab)}\n",
    "src_index_to_word = {i:w for i,w in enumerate(src_vocab)}\n",
    "trg_word_to_index = {w:i for i,w in enumerate(trg_vocab)}\n",
    "trg_index_to_word = {i:w for i,w in enumerate(trg_vocab)}\n",
    "'''\n",
    "class Vocab():\n",
    "    def __init__(self):\n",
    "         self.word_to_index = {'<sos>':0,'<eos>':1,'<pad>':2}\n",
    "         self.index_to_word = {0:'<sos>',1:'<eos>',2:'<pad>'}\n",
    "         self.word_count = 3\n",
    "    \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = self.word_count\n",
    "            self.index_to_word[self.word_count] = word\n",
    "            self.word_count += 1\n",
    "    \n",
    "    def add_sentence(self,sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word)\n",
    "\n"
   ],
   "id": "3039aa6d6c70f1c0",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:22.045425700Z",
     "start_time": "2024-11-23T11:08:21.166634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_vocab = Vocab()\n",
    "for sentence in src_sentences:\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        src_vocab.add_word(word)\n",
    "trg_vocab = Vocab()\n",
    "for sentence in trg_sentences:\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        trg_vocab.add_word(word)\n",
    "len(src_vocab.word_to_index),len(trg_vocab.word_to_index)"
   ],
   "id": "22ceb47ac25d2073",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 120)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:22.056396700Z",
     "start_time": "2024-11-23T11:26:17.710382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sentencetoindexes(sentence,vocab):\n",
    "    return [vocab.word_to_index[word] for word in nltk.word_tokenize(sentence)]\n",
    "sentencetoindexes(src_sentences[0],src_vocab)\n",
    "def make_data(src_sentences,trg_sentences,src_vocab,trg_vocab):\n",
    "    src_data = []\n",
    "    trg_data = []\n",
    "    for i,(src_sentence,trg_sentence) in enumerate(zip(src_sentences,trg_sentences)):\n",
    "        src_indexes = sentencetoindexes(src_sentence,src_vocab)\n",
    "        src_indexed = [sos_token] + src_indexes + [eos_token] + [pad_token]*(max_input_len-len(src_indexes))\n",
    "        trg_indexes = sentencetoindexes(trg_sentence,trg_vocab)\n",
    "        trg_indexed = [sos_token] + trg_indexes + [eos_token] + [pad_token]*(max_output_len-len(trg_indexes))\n",
    "        src_data.append(src_indexed)\n",
    "        trg_data.append(trg_indexed)\n",
    "    src_data = torch.LongTensor(src_data)\n",
    "    trg_data = torch.LongTensor(trg_data)\n",
    "    return src_data,trg_data\n",
    "src_data,trg_data = make_data(src_sentences,trg_sentences,src_vocab,trg_vocab)      \n",
    "src_data.shape,trg_data.shape\n",
    "src_batch = DataLoader(src_data,batch_size=batch_size)\n",
    "trg_batch = DataLoader(trg_data,batch_size=batch_size)\n",
    "for data in src_batch:\n",
    "    print(data.shape)\n",
    "    break"
   ],
   "id": "5978510d4ffc6bac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:22.061383700Z",
     "start_time": "2024-11-23T17:13:01.318380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,embedding_dim,hidden_dim):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim,embedding_dim) \n",
    "        self.rnn = nn.GRU(embedding_dim,hidden_dim,batch_first=True)\n",
    "    \n",
    "    def forward(self,src):\n",
    "        embedding = self.embedding(src)\n",
    "        #print(embedding.shape)\n",
    "        outputs,hidden = self.rnn(embedding)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,embedding_dim,hidden_dim):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim,embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim,hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim,output_dim)\n",
    "        self.vocab_size = output_dim\n",
    "    \n",
    "    \n",
    "    def forward_cell(self,input,hidden):\n",
    "        input = input.unsqueeze(0) #   (1,batch_size)\n",
    "        embedding = self.embedding(input) # (1,batch_size,embedding_dim)\n",
    "        #print('embedding of decoder 0.shape',embedding.shape)\n",
    "        output,hidden = self.rnn(embedding,hidden)# (1,batch_size,hidden_dim)\n",
    "        prediction = self.fc(output)\n",
    "        #print(\"pred\",prediction.shape,\"hidden\",hidden.shape)\n",
    "        return prediction,hidden\n",
    "    def forward(self,x,hidden,if_teacher_forcing):\n",
    "        #print(\"decoder input shape\",x.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        outputs = torch.zeros(seq_len,batch_size,self.vocab_size)\n",
    "        input = x[:,0] # (batch_size)\n",
    "        #print(\"decoder input 0.shape\",input.shape) \n",
    "        for i in range(1,max_output_len):\n",
    "            output,hidden = self.forward_cell(input,hidden) # (batch_size,output_dim)\n",
    "            output = output.squeeze(0)\n",
    "            outputs[i] = output\n",
    "            if(if_teacher_forcing):\n",
    "                input = x[:,i]\n",
    "            else:\n",
    "                input = torch.argmax(output,dim=1) # (batch_size)\n",
    "        return outputs\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,if_teacher_forcing):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.if_teacher_forcing = if_teacher_forcing\n",
    "        \n",
    "    def forward(self,src,trg):\n",
    "        hidden = self.encoder(src)\n",
    "        #print(\"encoder hiddem:\",hidden.shape)\n",
    "        outputs = self.decoder(trg,hidden,self.if_teacher_forcing)\n",
    "        return outputs.permute(1,2,0)\n",
    "    \n",
    "    def pred(self,src):\n",
    "        hidden = self.encoder(src)\n",
    "        outputs = self.decoder(src,hidden,self.if_teacher_forcing)\n",
    "        outputs = torch.argmax(outputs,dim=2)\n",
    "        print(outputs.shape)\n",
    "        return outputs\n",
    "    \n",
    "model = Seq2Seq(Encoder(len(src_vocab.word_to_index),256,512),Decoder(len(trg_vocab.word_to_index),256,512),True)\n",
    "for src,trg in zip(src_batch,trg_batch):\n",
    "    print(model(src,trg).shape)\n",
    "    break        \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ],
   "id": "7b0e66f2ba4092d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 120, 48])\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:22.073351400Z",
     "start_time": "2024-11-23T17:02:10.197047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epoch = 300\n",
    "for i in range(epoch):\n",
    "    for src,trg in zip(src_batch,trg_batch):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src,trg)\n",
    "        #print(output.shape)\n",
    "        l = loss(output,trg)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%100 == 0:\n",
    "            print(l.item())\n",
    "        \n",
    "    "
   ],
   "id": "f2b0d2392bd1cd44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.851202011108398\n",
      "0.30413660407066345\n",
      "0.3007989823818207\n",
      "0.30006179213523865\n",
      "0.2997521460056305\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predictions",
   "id": "3883c274ba6fa318"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T17:36:22.075345700Z",
     "start_time": "2024-11-23T17:34:55.461155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for src,trg in zip(src_batch,trg_batch):\n",
    "    for i in range(5):\n",
    "        print(\"src:\",[src_vocab.index_to_word[word.item()] for word in src[i]])\n",
    "        print(\"trg:\",[trg_vocab.index_to_word[word.item()] for word in trg[i]])\n",
    "        print(\"pred:\",[trg_vocab.index_to_word[word.item()] for word in model.pred(src)[:,i]])\n",
    "    break"
   ],
   "id": "198c3fe445f1aeea",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m src,trg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[43msrc_batch\u001B[49m,trg_batch):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc:\u001B[39m\u001B[38;5;124m\"\u001B[39m,[src_vocab\u001B[38;5;241m.\u001B[39mindex_to_word[word\u001B[38;5;241m.\u001B[39mitem()] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m src[i]])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'src_batch' is not defined"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
