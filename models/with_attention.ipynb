{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T21:57:11.006033Z",
     "start_time": "2024-11-21T21:57:10.999052Z"
    }
   },
   "source": [
    "# Importing the libraries\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import embedding\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T21:01:56.422653Z",
     "start_time": "2024-11-21T21:01:56.407691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_input_len = 30\n",
    "max_output_len = 48\n",
    "sos_token = 0\n",
    "eos_token = 1\n",
    "pad_token = 2\n",
    "batch_size = 16"
   ],
   "id": "f15106d1f3a2dcd",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:00:35.739323Z",
     "start_time": "2024-11-21T19:00:35.701294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"..\\dataset\\ELRC-417-Swedish_Work_Environ.en-fr.en\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "src_sentences = []\n",
    "#print(len(lines))\n",
    "for line in lines[1:-1]:\n",
    "    src_sentences.append(line.lower().strip())\n",
    "\n",
    "with open(r\"..\\dataset\\ELRC-417-Swedish_Work_Environ.en-fr.fr\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "trg_sentences = []\n",
    "for line in lines[1:-1]:\n",
    "    trg_sentences.append(line.lower().strip())"
   ],
   "id": "b513fb6c0c5c622e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:00:36.365688Z",
     "start_time": "2024-11-21T19:00:36.316038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.word_tokenize(src_sentences[0])\n",
    "max(len(nltk.word_tokenize(sentence)) for sentence in trg_sentences)"
   ],
   "id": "274bd48f562452b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:00:36.703708Z",
     "start_time": "2024-11-21T19:00:36.694314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "src_word_list = []\n",
    "for sentence in src_sentences:\n",
    "    src_word_list.extend([word.lower() for word in sentence[:-1].split()])\n",
    "#len(src_word_list)\n",
    "src_vocab = set(src_word_list)\n",
    "\n",
    "src_vocab = list(src_vocab)\n",
    "src_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\n",
    "trg_word_list = []\n",
    "for sentence in trg_sentences:\n",
    "    trg_word_list.extend([word.lower() for word in sentence[:-1].split()])\n",
    "trg_vocab = list(set(trg_word_list))\n",
    "trg_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\n",
    "trg_vocab\n",
    "'''"
   ],
   "id": "8181c86554511dfb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsrc_word_list = []\\nfor sentence in src_sentences:\\n    src_word_list.extend([word.lower() for word in sentence[:-1].split()])\\n#len(src_word_list)\\nsrc_vocab = set(src_word_list)\\n\\nsrc_vocab = list(src_vocab)\\nsrc_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\\ntrg_word_list = []\\nfor sentence in trg_sentences:\\n    trg_word_list.extend([word.lower() for word in sentence[:-1].split()])\\ntrg_vocab = list(set(trg_word_list))\\ntrg_vocab.extend(['<unk>','<pad>','<sos>','<eos>','.',','])\\ntrg_vocab\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "maybe try nltk.tokenize tomorrow",
   "id": "e1e2433d30df8280"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:00:36.777253Z",
     "start_time": "2024-11-21T19:00:36.763291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "src_word_to_index = {w:i for i,w in enumerate(src_vocab)}\n",
    "src_index_to_word = {i:w for i,w in enumerate(src_vocab)}\n",
    "trg_word_to_index = {w:i for i,w in enumerate(trg_vocab)}\n",
    "trg_index_to_word = {i:w for i,w in enumerate(trg_vocab)}\n",
    "'''\n",
    "class Vocab():\n",
    "    def __init__(self):\n",
    "         self.word_to_index = {'<sos>':0,'<eos>':1,'<pad>':2}\n",
    "         self.index_to_word = {0:'<sos>',1:'<eos>',2:'<pad>'}\n",
    "         self.word_count = 3\n",
    "    \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = self.word_count\n",
    "            self.index_to_word[self.word_count] = word\n",
    "            self.word_count += 1\n",
    "    \n",
    "    def add_sentence(self,sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word)\n",
    "\n"
   ],
   "id": "3039aa6d6c70f1c0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T19:00:36.870350Z",
     "start_time": "2024-11-21T19:00:36.857386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_vocab = Vocab()\n",
    "for sentence in src_sentences:\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        src_vocab.add_word(word)\n",
    "trg_vocab = Vocab()\n",
    "for sentence in trg_sentences:\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        trg_vocab.add_word(word)\n",
    "len(src_vocab.word_to_index),len(trg_vocab.word_to_index)"
   ],
   "id": "22ceb47ac25d2073",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T22:00:54.632938Z",
     "start_time": "2024-11-21T22:00:54.594914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sentencetoindexes(sentence,vocab):\n",
    "    return [vocab.word_to_index[word] for word in nltk.word_tokenize(sentence)]\n",
    "sentencetoindexes(src_sentences[0],src_vocab)\n",
    "def make_data(src_sentences,trg_sentences,src_vocab,trg_vocab):\n",
    "    src_data = []\n",
    "    trg_data = []\n",
    "    for i,(src_sentence,trg_sentence) in enumerate(zip(src_sentences,trg_sentences)):\n",
    "        src_indexes = sentencetoindexes(src_sentence,src_vocab)\n",
    "        src_indexed = [sos_token] + src_indexes + [eos_token] + [pad_token]*(max_input_len-len(src_indexes))\n",
    "        trg_indexes = sentencetoindexes(trg_sentence,trg_vocab)\n",
    "        trg_indexed = [sos_token] + trg_indexes + [eos_token] + [pad_token]*(max_output_len-len(trg_indexes))\n",
    "        src_data.append(src_indexed)\n",
    "        trg_data.append(trg_indexed)\n",
    "    src_data = torch.LongTensor(src_data)\n",
    "    trg_data = torch.LongTensor(trg_data)\n",
    "    return src_data,trg_data\n",
    "src_data,trg_data = make_data(src_sentences,trg_sentences,src_vocab,trg_vocab)      \n",
    "src_data.shape,trg_data.shape\n",
    "src_batch = DataLoader(src_data,batch_size=batch_size)\n",
    "trg_batch = DataLoader(trg_data,batch_size=batch_size)\n",
    "for data in src_batch:\n",
    "    print(data.shape)\n",
    "    break"
   ],
   "id": "5978510d4ffc6bac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T21:57:40.873379Z",
     "start_time": "2024-11-21T21:57:40.826504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,embedding_dim,hidden_dim):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim,embedding_dim) \n",
    "        self.rnn = nn.GRU(embedding_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,src):\n",
    "        embedding = self.embedding(src)\n",
    "        outputs,hidden = self.rnn(embedding)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,embedding_dim,hidden_dim):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim,embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim,hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim,output_dim)\n",
    "    \n",
    "    def forward_cell(self,input,hidden):\n",
    "        input = input.unsqueeze(0) #   (1,batch_size)\n",
    "        embedding = self.embedding(input) # (batch_size,1,embedding_dim)\n",
    "        output,hidden = self.rnn(embedding,hidden)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction,hidden\n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.shape[0]\n",
    "        outputs = torch.zeros(max_output_len,batch_size,output_dim)\n",
    "        input = x[:,0] # (batch_size)\n",
    "        for i in range(max_output_len):\n",
    "            output,hidden = self.forward_cell(input,hidden) # (batch_size,output_dim)\n",
    "            outputs[i] = output\n",
    "            input = torch.argmax(output,dim=1) # (batch_size)\n",
    "        return outputs\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,src,trg):\n",
    "        hidden = self.encoder(src)\n",
    "        outputs = self.decoder(trg,hidden)\n",
    "        return outputs\n",
    "\n",
    "model = Seq2Seq(Encoder(len(src_vocab.word_to_index),256,512),Decoder(len(trg_vocab.word_to_index),256,512))\n",
    "model(src_batch,trg_batch).shape\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ],
   "id": "7b0e66f2ba4092d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(114, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(120, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (fc): Linear(in_features=512, out_features=120, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T21:53:44.591924Z",
     "start_time": "2024-11-21T21:53:44.569983Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f2b0d2392bd1cd44",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
